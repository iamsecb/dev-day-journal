{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About","text":"<p>Daily record of musings, learnings, things to learn, gaps in knowledge and mistakes that lead to more learning \ud83d\udcd8\ud83e\udd14\ud83e\udd2f</p>"},{"location":"roadmap.mm/","title":"Roadmap","text":"<p>This is a collection of resources to skill up in Cloud and Backend Engineering.</p> <p>Note</p> <p>This list is a work in progress.</p> ---  title: Roadmap  markmap:    colorFreezeLevel: 3  ---  # Roadmap    This is a collection of resources to skill up in Cloud and Backend Engineering.    !!! note        This list is a work in progress.        {!roadmap.mm.md!}    ----      ## AWS    ### S3    - AWS: Deploy a Static Website to Amazon S3 `workshop:manning`    ### VPC    ### Databases    - AWS: Store On-Prem Data on DynamoDB `workshop;manning`    ### Serverless    - AWS: Migrate an Application to Serverless AWS `workshop:manning`    ### EC2    ### Cloudfront    ### IAM    - AWS: AWS Security Controls `workshop:manning`    ### R53      ## K8s    - Core Kubernetes `book`  - GitOps and Kubernetes `book`    - **EKS**      - AWS EKS Kubernetes-Masterclass `course:udemy`      - [EKS immersion day](https://catalog.workshops.aws/eks-immersionday/en-US) `course:aws`    - **Containers**      - [Learning Containers, Kubernetes, and Backend Development](https://iximiuz.com/) `blog`      ## Authentication &amp; Authorization    - External APIs: GitHub Sign-In Application `workshop`    ## Networking    ## Security    - Securing DevOps `book`      ## Distributed systems    - **System design**      - Build an Orchestrator in Go `book`      - Distributed Services with Go `book`      - [Distributed systems for fun and profit](http://book.mixu.net/distsys/single-page.html) `book`      - [High Scaleability](http://highscalability.com/) `blog`      - [needcode.io](https://neetcode.io)      - **Service mesh**      - Istio in Action `book`      ### Observability    - Cloud Observability in Action `book`    ## Software Architecture    - **System design**      - Rocking System Design `course:udemy`    - **Design patterns**      - Grokking Simplicity `book`    - **Mindset &amp; skill**         - The Creative Programmer `book`      - The Programmer's Brain `book`    - **Go**      - Effective Go `book`      - Learn Concurrent Programming with Go `book`      - 100 Go Mistakes and How to Avoid Them `book`      - Let's Go further `book`    - **Algorithms**      - Grokking Algorithms `book`      - A Common-Sense Guide to Data Structures and Algorithms `book`      - [neetcode.io](https://neetcode.io)      - External APIs: GitHub Data Management `workshop`"},{"location":"roadmap.mm/#aws","title":"AWS","text":""},{"location":"roadmap.mm/#s3","title":"S3","text":"<ul> <li>AWS: Deploy a Static Website to Amazon S3 <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#vpc","title":"VPC","text":""},{"location":"roadmap.mm/#databases","title":"Databases","text":"<ul> <li>AWS: Store On-Prem Data on DynamoDB <code>workshop;manning</code></li> </ul>"},{"location":"roadmap.mm/#serverless","title":"Serverless","text":"<ul> <li>AWS: Migrate an Application to Serverless AWS <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#ec2","title":"EC2","text":""},{"location":"roadmap.mm/#cloudfront","title":"Cloudfront","text":""},{"location":"roadmap.mm/#iam","title":"IAM","text":"<ul> <li>AWS: AWS Security Controls <code>workshop:manning</code></li> </ul>"},{"location":"roadmap.mm/#r53","title":"R53","text":""},{"location":"roadmap.mm/#k8s","title":"K8s","text":"<ul> <li>Core Kubernetes <code>book</code></li> <li> <p>GitOps and Kubernetes <code>book</code></p> </li> <li> <p>EKS</p> <ul> <li>AWS EKS Kubernetes-Masterclass <code>course:udemy</code></li> <li>EKS immersion day <code>course:aws</code></li> </ul> </li> <li> <p>Containers</p> <ul> <li>Learning Containers, Kubernetes, and Backend Development <code>blog</code></li> </ul> </li> </ul>"},{"location":"roadmap.mm/#authentication-authorization","title":"Authentication &amp; Authorization","text":"<ul> <li>External APIs: GitHub Sign-In Application <code>workshop</code></li> </ul>"},{"location":"roadmap.mm/#networking","title":"Networking","text":""},{"location":"roadmap.mm/#security","title":"Security","text":"<ul> <li>Securing DevOps <code>book</code></li> </ul>"},{"location":"roadmap.mm/#distributed-systems","title":"Distributed systems","text":"<ul> <li> <p>System design</p> <ul> <li>Build an Orchestrator in Go <code>book</code></li> <li>Distributed Services with Go <code>book</code></li> <li>Distributed systems for fun and profit <code>book</code></li> <li>High Scaleability <code>blog</code></li> <li>needcode.io</li> </ul> </li> <li> <p>Service mesh</p> <ul> <li>Istio in Action <code>book</code></li> </ul> </li> </ul>"},{"location":"roadmap.mm/#observability","title":"Observability","text":"<ul> <li>Cloud Observability in Action <code>book</code></li> </ul>"},{"location":"roadmap.mm/#software-architecture","title":"Software Architecture","text":"<ul> <li> <p>System design</p> <ul> <li>Rocking System Design <code>course:udemy</code></li> </ul> </li> <li> <p>Design patterns</p> <ul> <li>Grokking Simplicity <code>book</code></li> </ul> </li> <li> <p>Mindset &amp; skill </p> <ul> <li>The Creative Programmer <code>book</code></li> <li>The Programmer's Brain <code>book</code></li> </ul> </li> <li> <p>Go</p> <ul> <li>Effective Go <code>book</code></li> <li>Learn Concurrent Programming with Go <code>book</code></li> <li>100 Go Mistakes and How to Avoid Them <code>book</code></li> <li>Let's Go further <code>book</code></li> </ul> </li> <li> <p>Algorithms</p> <ul> <li>Grokking Algorithms <code>book</code></li> <li>A Common-Sense Guide to Data Structures and Algorithms <code>book</code></li> <li>neetcode.io</li> </ul> </li> <li> <p>External APIs: GitHub Data Management <code>workshop</code></p> </li> </ul>"},{"location":"insights/2023/05-05-2023/adapter-pattern/","title":"Adapter Pattern","text":"<p>An interesting article that describes the need for the adapter pattern.</p> <p>https://bitfieldconsulting.com/golang/adapter</p> <p>Some takeways:</p> <ul> <li> <p>As the article states \"Dependency expertise and business logic don\u2019t mix\" because it creates 2 problems; impacts   testability and violates the single responsibility principle by breaching the scope of interest for that particular function. In the example, this is represented by mixing implementation details of a database in a function that deals with business logic.</p> </li> <li> <p>An adapter abstracts the implementation details away from the business logic. </p> <p>In the above example, defining an interface solves the issue.This would allow us to completely decouple the need for a real endpoint.</p> <pre><code>type Store interface {\n    Store(Widget) (string, error)\n}\n</code></pre> </li> <li> <p>Test inputs and outputs.  </p> <p>With an adapter, you want to ensure the data supplied as input will produce a valid result. On the same note, the return value or the output is in the format you expect it to be in.  This can be difficult to test without running a real system (e.g: database). One solution to bridge this gap is to use a mocking library. Even this may have limitation as you are still mocking the data and the real data might be different. However, you eventually will need to perform an integration test with a real system. If if it succeeds, you have shifted-left by having higher confidence that your tests will make the program work as expected. If it fails, it helps to adjust your assumption about how you thought it should work and fix the tests accordingly.</p> </li> </ul>","tags":["api-design","go"]},{"location":"insights/2023/06-05-2023/api-design/","title":"API Design","text":"","tags":["api-design"]},{"location":"insights/2023/06-05-2023/api-design/#designing-a-simple-api-client","title":"Designing a simple API client","text":"<p>A weather client as per https://bitfieldconsulting.com/golang/api-client</p> <p>Some takeaways:</p> <ul> <li>Determine what to test by inputs and outputs.</li> <li>Put any mock data for tests in <code>testdata</code>.</li> <li>when dealing API responses, you must guard against results that are unexpectedly empty, incomplete, invalid, or has the wrong schema. </li> <li> <p>A quote worth remembering:</p> <p>Don\u2019t test other people\u2019s code: test that your code does the right thing when theirs doesn\u2019t.</p> </li> <li> <p>Avoid \"paperwork\". For example, why get something from one function just to pass it to another? The API should be   simple for a developer to consume.</p> </li> </ul>","tags":["api-design"]},{"location":"insights/2023/10-08-2023/database-index-constraints/","title":"Database Index and Constraints","text":"<p>While working on a project, I came across the need to know how constraints and indexes are related to each other. Here are a few key things to know.</p> <p>The COTS app I was upgrading was a patch version upgrade and required a database migration to be perfomed. Given that this was a patch version upgrade, I did not expect any schema changes. When I looked into the fixed issues they indicated:</p> <p>The index can be renamed (usually can happen when backup and restore tools by users that deliberately rename the index)</p> <p>When  attempting to delete the original index while doing a migration, it (silently) fails and the constraints still exist in database</p> <p>Some migrations then fail because they don\u2019t expect this index to be present</p> <p>So based on this a few questions come to mind:</p>","tags":["database"]},{"location":"insights/2023/10-08-2023/database-index-constraints/#why-does-renaming-an-index-not-also-update-the-constraints-to-point-to-the-new-index-name","title":"Why does renaming an index not also update the constraints to point to the new index name?","text":"<p>The reason is that indexes and constraints are separate database objects that are related but independent of each other.</p> <p>When an index is renamed, the name change only applies to the index itself. Any constraints that reference the original index name are not automatically updated to point to the new name. This is by design, as renaming an index should not cause implicit/silent schema changes to constraints.</p> <p>Backup tools that rename indexes are likely doing so for identification/labeling purposes during the backup/restore process. But the database itself treats indexes and constraints as separate objects that reference each other by name. So a rename on the index side does not propagate or cascade an update to constraints that reference it.</p> <p>This can subsequently cause issues, as was seen in this case, if migrations expect the original index name but get the renamed one instead. The constraints are still pointing to the old name.</p>","tags":["database"]},{"location":"insights/2023/10-08-2023/database-index-constraints/#what-is-the-relationship-between-an-index-and-contrainsts","title":"What is the relationship between an Index and Contrainsts?","text":"<p>Before discussing the relationship between indexes and constraints, it's important to understand what each one is:</p> <p>Constrainsts are rules placed on the data to enforce integrity. Constraints are applied at the column or table level and prevent invalid data from being inserted or updated. Common constraints include primary keys, foreign keys, not null, unique, check constraints etc.</p> <p>An Index is defined as a way to quickly locate and access records in a database table. Indexes are data structures that contain keys built from one or more columns in a table, along with pointers to the actual row locations. Indexes are used to speed up queries and sorts on columns they are built for.</p> <p>Now that there is a foundational understanding of each, here is the relationship between indexes and constraints:</p> <p>Foreign key and unique constraints often utilize indexes internally to efficiently validate the data. When a foreign key or unique constraint is created, the database will automatically create a matching index if one does not already exist. This index supports the validation performed by the constraint. </p> <p>Let's consider a scenario where you have two tables, \"Orders\" and \"Customers.\" The \"Orders\" table has a foreign key referencing the \"Customers\" table to maintain the relationship between orders and their corresponding customers. To optimize lookup operations based on customer IDs, an index is created on the foreign key column in the \"Orders\" table. </p> <p>If the index is renamed, the foreign key constraint in the \"Orders\" table will still reference the original index name. So if you have a database migration that expects to delete original index and remove references to the index in any constraints, it will fail or produce unexpected results since the constraint is still pointing to the old index name. The constraint and index remain out of sync after the index rename.</p>","tags":["database"]},{"location":"insights/2023/17-04-2023/multi-git-profiles/","title":"Multiple Git Profiles","text":"<p>Needed to setup multiple git profiles e.g: work and personal. Discovered that there is a <code>includeIf</code> section that can be used in the <code>.gitconfig</code> file to split profile spcific information like <code>user</code>.</p> <ul> <li>Reference </li> <li>Example</li> </ul>","tags":["git"]},{"location":"insights/2023/18-05-2023/linux-primer/","title":"Linux Primer","text":"","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#insights","title":"Insights","text":"","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-the-linux-kernel","title":"What is the linux kernel?","text":"<ul> <li>Controls the hardware when the OS talks to the kernel via system calls. The    kernel translates these requests into instructions that the hardware can    understand. </li> <li>Allocates memory and schedules processes to run applications.</li> <li>First piece of software loaded into a protected area of memory when a computer starts up so that it cannot be overwritten.</li> </ul> <p>In the command <code>ps -ef</code> PID 1 is the initial process started by the kernel. </p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-a-system-call","title":"What is a system call?","text":"<p>When an application is run, by default it runs in user space. When it requires access to hardware like disk for example it must make a request to the kernel which is known as a \"system call\". </p> <p>Here are some common scenarios where applications make system calls to the kernel:</p> <p>File operations: When an application needs to read from or write to files, it makes system calls to the kernel to perform file-related operations, such as opening files, reading data, writing data, closing files, and modifying file attributes.</p> <p>Network communication: Applications that require network connectivity, such as web browsers or email clients, make system calls to the kernel to establish network connections, send data over the network, receive incoming data, and manage network sockets.</p> <p>Process management: Applications may need to create new processes, terminate processes, or perform other process-related operations. These tasks involve system calls to the kernel, which handles process scheduling, memory management, and inter-process communication.</p> <p>Memory management: When an application requires memory allocation or deallocation, it relies on system calls to the kernel to request memory resources. The kernel manages the system's memory and fulfills these requests, ensuring proper memory allocation and protection.</p> <p>Interacting with devices: Applications make system calls to interact with hardware devices like disks, printers, graphics cards, and input/output devices. These system calls enable the application to perform operations on the devices with the assistance of the kernel and relevant device drivers.</p> <p>If you run an application as root, it still runs within the user space but is granted elevated privilges that normally would not exist like modifying system files, changing system level configuration etc.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#files","title":"Files","text":"<ul> <li><code>/bin</code>, <code>/sbin</code>, <code>/usr/bin</code>, and <code>/usr/sbin</code>: Where executable programs are stored.</li> <li><code>/dev</code>: Where files representing hardware devices are stored. For example, if your Linux system had a floppy drive device, there would be a file named fd0 in the dev folder (/dev/fd0).</li> <li><code>/etc</code>: Where configuration files are stored.</li> <li><code>/home</code>: Where user home directories are stored, one for each user.</li> <li><code>/var</code>. Where variable-length files, like log files, are stored.</li> </ul> <p>Should follow File System Hierachy guide as per https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#log-files","title":"Log files","text":"<p><code>syslog</code>: Contains the centralized logging system, called syslog, in which you\u2019ll find messages related to the kernel, applications, and more. If configured, this could be the centralized log file for all Linux systems (or even all network devices) in your data center.</p> <p><code>auth.log</code>: Contains authentication failures and successes</p> <p><code>messages</code>: Contains general system messages of all types</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#network-interfaces","title":"Network interfaces","text":"<p>The loopback (<code>lo</code>) interface will have an IP address of 127.0.0.1, which represents the host itself.</p> <p>The ethernet 0 (<code>eth0</code>) interface is typically the connection to the local network. Even if you are running Linux in a virtual machine (VM), you\u2019ll still have an <code>eth0</code> interface that connects to the physical network interface of the host. Most commonly, you should ensure that eth0 is in an UP state and has an IP address so that you can communicate with the local network and likely over the Internet.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#commands","title":"Commands","text":"<p><code>ip link</code>         : Configure network interfaces and check link status  <code>ip addr ...</code>     : Check and configure ip addresses for network interfaces <code>ip -s link</code>      : Stats on our network e.g: How much data is sent? any errors? etc. <code>netstat -l</code>      : Active listening services <code>ip neighbour</code>    : ARP cache table (IP to MAC) <code>ifup/ifdown</code> .   : Restart an interfaces without having to restart a servero</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#container-networking-made-simple","title":"Container networking made simple","text":"<p>The following are notes from the article https://iximiuz.com/en/posts/container-networking-is-simple/ (which is a must read).</p> <p><sub>Disclaimer: The notes are my own and if there are mistakes it is not relfective of the article.</sub></p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#what-is-a-linux-network-stack","title":"What is a linux network stack?","text":"<p>An isolated network device, routing rules and any filters set by ip tables. The isolation provided by linux via a network namespace can be setup via the command <code>ip netns</code>.</p> <p>The man page for this command says:</p> <p>A network namespace is logically another copy of the network stack, with its own routes, firewall rules, and network devices.</p> <p>This is captured in the following script:</p> <p><pre><code>cat &lt;&lt;EOF &gt; inspect-net-stack.sh\n#!/usr/bin/env bash\necho \"&gt; Network devices\"\nip link\n\necho -e \"\\n&gt; Route table\"\nip route\n\necho -e \"\\n&gt; Iptables rules\"\niptables --list-rules\nEOF\n</code></pre> Set permissions:</p> <pre><code>chmod +x inspect-net-stack.sh\n</code></pre> <p>Create a custom ip tables rule to see the difference between the root network namespace and the custom namespace that will be created.</p> <pre><code>sudo iptables -N NS_ROOT\n</code></pre> <p>When you run this on the host (root network namespace) on a machine the output may look like this:</p> <pre><code>&gt; Network devices\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n\n&gt; Route table\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N ROOT_NS\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-to-create-a-network-namespace","title":"How to create a network namespace?","text":"<pre><code>$ sudo ip netns add netns0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-do-i-use-this-namespace","title":"How do I use this namespace?","text":"<pre><code># Run bash in the namespace we created\n$ sudo nsenter --net=/var/run/netns/netns0 bash\n\n$ sudo ./inspect-net-stack.sh\n&gt; Network devices\n1: lo: &lt;LOOPBACK&gt; mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n\n&gt; Route table\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n</code></pre> <p>As you can see there is only a loopback interface (which is also <code>DOWN</code>), no routing rules and the custom ip tables chain <code>NS_ROOT</code> is not present which confirms that we are in the <code>netns0</code> networking stack.</p> <p>At this point it is completely isolated and there is no network connectivity by default.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#how-to-make-this-network-namespace-useful","title":"How to make this network namespace useful?","text":"<p>To enable connectivity to the <code>netns0</code> namepace, we need to create a form of link to the root namespace. Linux provides a way to make this happen via virtual ethernet devices.</p> <p>The man page for <code>veth</code> says:</p> <p>The  veth devices are virtual Ethernet devices.  They can act as tunnels between network namespaces to create a bridge to a physical network device in another namespace, but can also be used as standalone network devices.</p> <p>Create a <code>veth</code> on the root networking namespace:</p> <pre><code>$ sudo ip link add veth0 type veth peer name ceth0\n</code></pre> <p>The command highlights that we are creating the veth in pairs; veth0/ceth0. The purpose of creating veth interfaces in pairs is to establish a communication channel between different network namespaces. One end of the veth pair is typically assigned to the host's network namespace, while the other end is assigned to a specific network namespace, such as a container or another network namespace.</p> <p>Running the network commands from earlier now shows:</p> <pre><code>sudo ./inspect-net-stack.sh\n&gt; Network devices\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n3: ceth0@veth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether da:c7:10:9f:f7:3c brd ff:ff:ff:ff:ff:ff\n4: veth0@ceth0: &lt;BROADCAST,MULTICAST,M-DOWN&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff\n\n&gt; Route table\ndefault via 10.0.2.2 dev eth0 proto dhcp metric 100\n10.0.2.0/24 dev eth0 proto kernel scope link src 10.0.2.15 metric 100\n\n&gt; Iptables rules\n-P INPUT ACCEPT\n-P FORWARD ACCEPT\n-P OUTPUT ACCEPT\n-N ROOT_NS\n</code></pre> <p>To connect the root networking namespace to the <code>netns0</code> namespace we need to move one of the pairs over:</p> <pre><code>$ sudo ip link set ceth0 netns netns0\n\n# Confirm it is not longer in the root namespace\n$ ip link show\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP mode DEFAULT group default qlen 1000\n    link/ether 52:54:00:27:8b:50 brd ff:ff:ff:ff:ff:ff\n4: veth0@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff link-netns netns0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#enabling-connectivity","title":"Enabling connectivity","text":"<p>From the root networking namespace we can see that the device is <code>DOWN</code> and there is no IP address assigned:</p> <pre><code>4: veth0@if3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000\n    link/ether 8a:09:3b:a1:50:5d brd ff:ff:ff:ff:ff:ff link-netns netns0\n</code></pre> <p>We can fix that:</p> <pre><code>sudo ip link set veth0 up\nsudo ip addr add 172.18.0.11/16 dev veth0\n</code></pre> <p>IP Addressing</p> <p>Any ip address can be assigned to the interface because it is virtual. It doesn't have to be a real IP address that corresponds to a physical network device.</p> <p>LOWER_LAYER_DOWN</p> <p>If you had run <code>ip link show veth0</code>, you will notice that the link state is <code>LOWERLAYERDOWN</code>. This is expected   since the other end of the pair, <code>ceth0</code> is down.</p> <p>Doing the same for <code>ceth0</code> in <code>netns0</code> namespace:</p> <pre><code>sudo nsenter --net=/var/run/netns/netns0\nip link set lo up\nip link set ceth0 up\nip addr add 172.18.0.10/16 dev ceth0\n</code></pre> <p>Review connectivity:</p> <pre><code>$ ip link\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n3: ceth0@if4: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default qlen 1000\n    link/ether 82:95:96:e2:c0:00 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#testing-connectivity","title":"Testing connectivity","text":"<p>From <code>netns0</code> namespace to <code>veth0</code>:</p> <p><pre><code>sudo nsenter --net=/var/run/netns/netns0\n</code></pre> Ping <code>veth0</code> in host namespace:</p> <pre><code>$ ping -c 2 172.18.0.11\nPING 172.18.0.10 (172.18.0.10) 56(84) bytes of data.\n64 bytes from 172.18.0.10: icmp_seq=1 ttl=64 time=0.073 ms\n64 bytes from 172.18.0.10: icmp_seq=2 ttl=64 time=0.046 ms\n...\n</code></pre> <p>Make sure to exit from <code>ns0</code> namespace:</p> <pre><code>exit\n</code></pre> <p>From root namespace to <code>ceth0</code>:</p> <pre><code>$ ping -c 2 172.18.0.10\nPING 172.18.0.11 (172.18.0.11) 56(84) bytes of data.\n64 bytes from 172.18.0.11: icmp_seq=1 ttl=64 time=0.038 ms\n64 bytes from 172.18.0.11: icmp_seq=2 ttl=64 time=0.040 ms\n...\n</code></pre> <p>From <code>netns0</code> to root namespace <code>eth0</code> interface:</p> <pre><code># Get eth0 ip\n$ ip addr show dev eth0\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000\n    link/ether 52:54:00:e3:27:77 brd ff:ff:ff:ff:ff:ff\n    inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0\n       valid_lft 84057sec preferred_lft 84057sec\n    inet6 fe80::5054:ff:fee3:2777/64 scope link\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Switch to <code>ns0</code> namespace:</p> <pre><code>sudo nsenter --net=/var/run/netns/netns0\n</code></pre> <pre><code># Try host's eth0\n$ ping 10.0.2.15\nconnect: Network is unreachable\n\n# Try something from the Internet\n$ ping 8.8.8.8\nconnect: Network is unreachable\n</code></pre> <p>To understand why you can't ping the host's <code>eth0</code> interface you, we need to verify if a route exists. </p> <pre><code>$ ip route\n172.18.0.0/16 dev ceth0 proto kernel scope link src 172.18.0.10\n</code></pre> <p>There is only a route to reach <code>172.18.0.0/16</code> network. To fix this we can add a default route:</p> <pre><code>ip route add default via 172.18.0.11\n</code></pre> <p>Default Route</p> <p>The default route is the next hop. It is not an interface on your host. Think about a router in your home network for example to reach the internet.</p> <p>Now if ping <code>eth0</code> it will have a way to reach it:</p> <pre><code>$ ping -c2 10.0.2.15\nPING 10.0.2.15 (10.0.2.15) 56(84) bytes of data.\n64 bytes from 10.0.2.15: icmp_seq=1 ttl=64 time=0.040 ms\n64 bytes from 10.0.2.15: icmp_seq=2 ttl=64 time=0.062 ms\n...\n</code></pre> <p>However, accessing the internet as per our <code>ping 8.8.8.8</code> command still fails.</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#nat","title":"NAT","text":"<p>The article linked above describes it best:</p> <p>Before going to the external network, packets originated by the containers will get their source IP addresses replaced with the host's external interface address. The host also will track all the existing mappings and on arrival, it'll be restoring the IP addresses before forwarding packets back to the containers.</p> <p>This is achieved by the following <code>iptables</code> rule:</p> <pre><code>sudo sysctl net.ipv4.ip_forward=1\nsudo iptables -t nat -A POSTROUTING -s 172.18.0.0/16 -o eth0 -j MASQUERADE\n</code></pre> <p>This rule ensures that any packet originating from the <code>172.18.0.0/16</code> network and going out through the <code>eth0</code> interface will have its source IP address changed to match the IP address of the <code>eth0</code> interface. This allows devices in the <code>172.18.0.0/16</code> network to communicate with the internet using the IP address of the <code>eth0</code> interface.</p> <p>To be continued..</p>","tags":["networking","linux"]},{"location":"insights/2023/18-05-2023/linux-primer/#summary","title":"Summary","text":"<pre><code>sudo ip netns add netns0\nsudo ip link add veth0 type veth peer name ceth0\nsudo ip link set veth0 up\nsudo ip addr add 172.18.0.11/16 dev veth0\nsudo ip link set ceth0 netns netns0\nsudo nsenter --net=/var/run/netns/netns0\nip link set lo up\nip link set ceth0 up\nip addr add 172.18.0.10/16 dev ceth0\nip route add default via 172.18.0.11\nexit\n\nsudo ip netns add netns1\nsudo ip link add veth1 type veth peer name ceth1\nsudo ip link set veth1 up\nsudo ip addr add 172.18.0.21/16 dev veth1\nsudo ip link set ceth1 netns netns1\n\nsudo nsenter --net=/var/run/netns/netns1\nip link set lo up\nip link set ceth1 up\nip addr add 172.18.0.20/16 dev ceth1\nip route add default via 172.18.0.21\n# my host eth0 ip is 10.0.2.15\narping -c 1 -I ceth1 10.0.2.15\nexit\n\nsudo ip route del 172.18.0.0/16 dev veth0 proto kernel scope link src 172.18.0.11\nsudo nsenter --net=/var/run/netns/netns1\narping -c 1 -I ceth1 10.0.2.15\n</code></pre>","tags":["networking","linux"]},{"location":"insights/2023/19-04-2023/context-timeout/","title":"Go Context Timeout","text":"","tags":["go","client","context"]},{"location":"insights/2023/19-04-2023/context-timeout/#when-does-the-contexcontext-cancel-for-withtimeout","title":"When does the <code>contex.Context</code> cancel for <code>WithTimeout()</code> ?","text":"<p>If you have <code>funcA()</code> calling <code>funcB()</code> with a context timeout, the timer starts from the time the context is created and you will get a <code>DeadlineExceeded</code> error if the timer expires.</p> <p>typically you add a <code>defer cancel()</code> to ensure there is no context leak. If you are calling <code>funcB()</code> via a goroutine you must wrap the <code>defer cancel()</code> inside the goroutine. Otherwise, you will get <code>DeadlineExceeded</code> error.</p> <p>Example</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"time\"\n)\n\nfunc funcA() {\n    ctx, cancel := context.WithTimeout(context.Background(), time.Second*3)\n    go func() {\n        defer cancel() // Defer canceling the context\n        funcB(ctx)\n    }()\n}\n\nfunc funcB(ctx context.Context) {\n    select {\n    case &lt;-time.After(2 * time.Second):\n        // Do something that takes more than 2 seconds\n        fmt.Println(\"Work for 2 seconds\")\n    case &lt;-ctx.Done():\n        // Context canceled or timed out\n        if ctx.Err() == context.DeadlineExceeded {\n            fmt.Println(context.DeadlineExceeded)\n        } else {\n            fmt.Println(ctx.Err())\n        }\n    }\n\n}\nfunc main() {\n    funcA()\n    time.Sleep(time.Second * 3)\n\n}\n</code></pre>","tags":["go","client","context"]},{"location":"insights/2023/19-04-2023/reusing-http-client/","title":"Go Reusing HTTP Client","text":"","tags":["api-design","go","client"]},{"location":"insights/2023/19-04-2023/reusing-http-client/#clear-apis-and-reusing-the-http-client","title":"Clear APIs and reusing the http client","text":"<p>When designing an API it is critical to consider the usability of the API from the user's or developer's perspective. With this requirement in mind, when designing the usage of a HTTP client for a service, which may have  multiple sub-services, it is useful to provid a scoped API yet reuse the same HTTP client.</p> <p>I was looking for a pattern to do this and found the following library solving this in an interesting way:</p> <p>https://github.com/raksul/go-clickup/blob/main/clickup/client.go</p> <p>Let's look at the interesting bits.</p> <p>Declartion in <code>NewClient()</code> with my comments:</p> <pre><code>// Define a client. Nothing new here.\nc := &amp;Client{client: httpClient, BaseURL: baseURL, UserAgent: userAgent, APIKey: APIKey}\n// Now assign the client to common.\nc.common.client = c\n// We don't want to allocate to the heap so we assign the address of c.common to c.Attachments after casting\n// it as a pointer to AttachmentsService.\n// This means that c.Attachments now references the same object in memory as c.common.\nc.Attachments = (*AttachmentsService)(&amp;c.common)\n// Do the same for all the sub-services.\nc.Authorization = (*AuthorizationService)(&amp;c.common)\nc.Checklists = (*ChecklistsService)(&amp;c.common)\nc.Comments = (*CommentsService)(&amp;c.common)\n...\n</code></pre> <p>Corresponding defnitions for above code:</p> <pre><code>type service struct {\n        client *Client\n}\n\n\ntype AttachmentsService service\n\n\ntype Client struct {\n        clientMu sync.Mutex   // clientMu protects the client during calls that modify the CheckRedirect func.\n        client   *http.Client // HTTP client used to communicate with the API.\n        APIKey   string\n\n        BaseURL   *url.URL\n        UserAgent string\n\n        rateMu     sync.Mutex\n        rateLimits Rate // Rate limits for the client as determined by the most recent API calls.\n\n        common service // Reuse a single struct instead of allocating one for each service on the heap.\n\n        // Services used for talking to different parts of the Clickup API.\n        Attachments     *AttachmentsService\n        Authorization   *AuthorizationService\n        Checklists      *ChecklistsService\n        Comments        *CommentsService\n        ...\n}\n</code></pre> <p>This can be illustrated as shown below (for <code>Attachments</code>):</p> <p></p> <p>The takeaway being you can typecast and assign the memory address of <code>common</code> to <code>Attachments</code> because they have the same <code>Client</code> type.</p> <p>The API is now accessible as shown in the following pseudocode:</p> <pre><code>c := NewClient()\nc.Attachments.* // Attachments specific APIs \nc.Comments.* // Comments specific APIs\n</code></pre>","tags":["api-design","go","client"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/","title":"Istio Egress TLS Origination","text":"","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#overview","title":"Overview","text":"<p>Illustrate how to configure Istio to handle egress traffic to external services for path based routing setup via a Gateway.</p> <pre><code>Client (HTTPS) LB - SSL offload   --&gt; (HTTP) Istio-ingressgateway  --&gt; (HTTPS) External-service\n                                                                   --&gt; (HTTP)  Internal-service\n</code></pre> <p>When the incoming request to Istio is HTTP (rather than HTTPS), Istio does not automatically upgrade the request to HTTPS when connecting to the external service. This is also briefly mentioned in the documentation here:</p> <p>TLS origination occurs when an Istio proxy (sidecar or egress gateway) is configured to accept unencrypted internal HTTP connections, encrypt the requests, and then forward them to HTTPS servers that are secured using simple or mutual TLS.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#serviceentry","title":"ServiceEntry","text":"<p>Setup a ServiceEntry to enable access to n S3 bucket. The S3 bucket in this instance is our external service. This is a common configuration for accessing external services from Istio.</p> <p>Info</p> <p>The S3 bucket in this example has been configured to serve requests on paths <code>/.well-known/openid-configuration</code> and <code>/oauth/discovery/keys</code>.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: ServiceEntry\nmetadata:\n  name: s3\n  namespace: istio-system\nspec:\n  hosts:\n    - samplebucket1234.s3.ap-southeast-2.amazonaws.com\n  location: MESH_EXTERNAL\n  exportTo:\n  - \".\"\n  ports:\n    - number: 443\n      name: https\n      protocol: HTTPS\n  resolution: DNS\n</code></pre>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#gateway","title":"Gateway","text":"<p>Setup a Gateway that accepts HTTP traffic. It is SSL offloaded at the LB. </p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: Gateway\nmetadata:\n  name: example-gateway\nspec:\n  selector:\n    istio: ingressgateway  # use Istio default gateway implementation\n  servers:\n    - port:\n        number: 80\n        name: http\n        protocol: HTTP\n      hosts:\n        - example.com\n</code></pre>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#virtualservice","title":"VirtualService","text":"<p>In this example, the root path <code>/</code> requests should route to the <code>welcome</code> internal service  and <code>/.well-known/openid-configuration</code> and <code>/oauth/discovery/keys</code> to the external service. To provide this traffic routing a VirtualService is configured as shown below.</p> <pre><code>apiVersion: networking.istio.io/v1alpha3\nkind: VirtualService\nmetadata:\n  name: s3\nspec:\n  gateways:\n  - example-gateway\n  http:\n    - match:\n        - uri:\n            prefix: /\n      route:\n        - destination:\n            host: welcome  # Name of the internal service\n            port:\n              number: 8080\n    - match:\n        - uri:\n            prefix: /.well-known/openid-configuration \n      route:\n        - destination:\n            host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n            port:\n              number: 443\n      headers:\n        request:\n          set:\n            X-Forwarded-Proto: https\n            Host: samplebucket1234.s3.ap-southeast-2.amazonaws.com          \n    - match:\n        - uri:\n            prefix: /oauth/discovery/keys\n      route:\n        - destination:\n            host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n            port:\n              number: 443\n      headers:\n        request:\n          set:\n            X-Forwarded-Proto: https\n            Host: samplebucket1234.s3.ap-southeast-2.amazonaws.com          \n</code></pre> <p>At this point if you try to access the external services i.e: <code>example.com/.well-known/openid-configuration</code> or <code>example.com/oauth/discovery/keys</code> you will get the following error:</p> <pre><code>upstream connect error or disconnect/reset before headers. reset reason: protocol error\n</code></pre> <p>This is because the external service is expecting the request to be HTTPS. Istio does not perform TLS origination automatically. This has to be configured explicitly via a DestinationRule.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#destinationrule","title":"DestinationRule","text":"<p>Setup a DestinationRule to perform TLS origination for HTTPS requests on port 443.</p> <pre><code>---\napiVersion: networking.istio.io/v1alpha3\nkind: DestinationRule\nmetadata:\n  name: s3\n  namespace: istio-system\nspec:\n  host: samplebucket1234.s3.ap-southeast-2.amazonaws.com\n  exportTo:\n  - \".\"\n  trafficPolicy:\n    portLevelSettings:\n    - port:\n        number: 443\n      tls:\n        mode: SIMPLE # initiates HTTPS when accessing samplebucket1234.s3.ap-southeast-2.amazonaws.com\n</code></pre> <p>The <code>tls.mode</code> setting when set to <code>SIMPLE</code> will initiate a HTTPS connection to the external service.</p>","tags":["istio","tls"]},{"location":"insights/2023/20-09-2023/istio-egress-tls-origination/#references","title":"References","text":"<p>-https://istio.io/latest/docs/tasks/traffic-management/egress/egress-tls-origination/</p>","tags":["istio","tls"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/","title":"SSL CA Certs","text":"","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#faq","title":"FAQ","text":"","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#does-the-server-send-the-intermediate-ca-certs-in-the-ssl-handshake","title":"Does the server send the intermediate CA certs in the SSL handshake?","text":"<p>Short answer: Yes.</p> <p>See this StackOverflow answer.</p> <p>As per the RFC:</p> <pre><code>certificate_list\n  This is a sequence (chain) of certificates.  The sender's\n  certificate MUST come first in the list.  Each following\n  certificate MUST directly certify the one preceding it.  Because\n  certificate validation requires that root keys be distributed\n  independently, the self-signed certificate that specifies the root\n  certificate authority MAY be omitted from the chain, under the\n  assumption that the remote end must already possess it in order to\n  validate it in any case.\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#is-the-root-ca-cert-also-sent-by-the-server","title":"Is the Root CA cert also sent by the server?","text":"<p>It may not. The RFC says:</p> <p>The self-signed certificate that specifies the root certificate authority MAY be omitted from the chain, under the assumption that the remote end must already possess it in order to validate it in any case.</p> <p>So the client must have the Root CA cert installed in its trust store. Typically, all major Root CA  certificates are bundled into the client.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#how-do-i-view-the-certs-sent-by-the-server","title":"How do I view the certs sent by the server?","text":"<pre><code>openssl s_client -connect google.com:443 -showcerts &lt; /dev/null 2&gt;/dev/null\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#for-the-server-to-send-the-intermediate-ca-certs-does-it-need-to-be-configured-to-do-so","title":"For the server to send the intermediate CA certs, does it need to be configured to do so?","text":"<p>Yes.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#what-is-the-default-path-to-the-intermediate-ca-certs","title":"What is the default path to the intermediate CA certs?","text":"<p>The default path is <code>/etc/ssl/certs/ca-certificates.crt</code> on Debian and Ubuntu.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#what-is-the-chain-of-trust","title":"What is the Chain of Trust?","text":"<p>In the SSL handshake, when the browser receives the server's certificate, it needs to check if the certificate is signed by a trusted CA. The process of how this happens is outlined below.</p> <ol> <li>During the handshake, the client looks at the leaf certificate, which is typically signed by an intermediate CA.</li> <li>The browser will look for the public cert of the intermediate CA to verify the signature of the leaf certificate. If it finds it, it can complete signature verification.</li> <li>If the intermediate CA's public key is not found in the trust store, the browser must get the intermediate CA cert via the certificate list in the leaf cert by looking at the issuer. </li> <li>The browser would now have to verify the signature for the intermediate CA (via the parent CA that signed the intermediate CA cert) to complete the chain of trust. The browser can identify the issuer of the intermediate CA cert by searching in its trust store for the public cert of the issuer. If it finds it, it will use it to verify the signature of the intermediate CA cert which completes the chanin of trust.</li> <li>If it is not found in the trust store, step 3-4 will be repeated. This process stops when the browser either has the public key of the top-level intermediate CA that signed the intermediate CA to the leaf certificate or the issuer is the Root CA and verifies the signature because it has the Root CA public key in its trust store.</li> </ol> <p>If any step fails in this process (e.g., a certificate is expired, revoked, or the signature is invalid), the client will not trust the certificate, and the connection may be terminated or a warning message displayed.</p>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#how-do-you-retrieve-the-certificates-from-a-domain-programatically","title":"How do you retrieve the certificates from a domain programatically?","text":"<pre><code>openssl s_client -showcerts -connect  foo.com -servername  foo.com  &lt;/dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' &gt; foo.com.pem\n</code></pre>","tags":["ssl","faq"]},{"location":"insights/2023/22-09-2023/ssl-ca-certs-faq/#what-is-the-difference-between-signing-a-certificate-and-encrypting","title":"What is the difference between signing a certificate and encrypting?","text":"<p>They are related in that they use the same encryption algorithm e.g: RS256. </p> <p>However they serve different purposes.</p> <p>Signing: Used to maintain integrity and authenticity when the CA signs a certificate or an OIDC server signs a token.  The data is hashed and then signed by the server's private key. The client or relying party uses the public key to expose the hashed data by decrypting the signature or message. When we refer to \"signing\", we generally mean to use a private key to sign a piece of data that can be extracted via the public key.</p> <p>Encryption: Use to maintain confidentiality where the client encrypts the data using the server's public key (available to the client) that is then decrypted by the server's private key. </p>","tags":["ssl","faq"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/","title":"Hashicorp Vault Keys for Autounseal","text":"<p>The first thing to remember is that when Vault starts, it is in a Sealed state. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-does-sealed-state-mean","title":"What does Sealed state mean?","text":"<p>By configuration Vault knowns where and how to access the physical storage of secrets but cannot decrypt them. So it must be unsealed. This would mean it is encrypted in someway. Spot on, it is formally referred to as the Root key.</p> <p>This leads us back to unsealing, which is the process of getting access to the plaintext Root key. And why to do we need the Root key? because all data is encrypted with an Encryption key that is derived from the Root key.</p> <p>Unsealing is the process of getting access to the plaintext Root key.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#ok-so-where-does-the-root-key-come-from","title":"Ok, So where does the Root key come from?","text":"<p>The root key is stored alongside other vault data. Which means from a security standpoint, it needs to be encrypted at rest. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#so-theres-another-key-to-encrypt-the-root-key","title":"So there's another key to encrypt the Root key?","text":"<p>Yes, this is where the Unseal Key is relevant. The auto unseal feature delegates responsibility of securing the Unseal key to a trusted device or system. At startup Vault will connect to the device or system and ask it to decrypt the Root key it read from storage.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-is-the-root-token-then","title":"What is the Root token then?","text":"<p>Root tokens are tokens that have the root policy attached to them. Root tokens can do anything in Vault. Anything.</p> <p>There are 3 ways root token can exist:</p> <ol> <li>When vault is initialised via <code>vault operation init</code></li> <li>Using an existing root token to create another </li> <li>By running <code>vault operator generate-root</code> using the Recovery keys</li> </ol>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#what-are-recovery-keys","title":"What are Recovery keys?","text":"<p>There are certain operations in Vault that require explicit authorization for it to be performed. For example, unsealing Vault or generating a Root token. When using Autounseal, this requires Recovery keys. </p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#how-do-we-get-the-recovery-keys","title":"How do we get the Recovery keys?","text":"<p>When Vault is first initialised via Autounseal, it yield Recovery keys from the Root key. In other words, the Root key is split into a series of key shares following Shamir's Secret Sharing Algorithm.</p> <p>One of the first operational activities is to initialise Vault by running <code>vault operator init</code>. This will generate the Root key, the Recovery keys (as desribed) and the Root token.</p> <p>The process of generating a new Root key is called Rekeying.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#explain-rekeying","title":"Explain Rekeying?","text":"<p>The process for generating a new root key and applying Shamir's algorithm is called \"rekeying\"</p> <pre><code>$vault operator rekey -target=recovery -init -key-shares=5 -key-threshold=3\n\nWARNING! If you lose the keys after they are returned, there is no recovery.\nConsider canceling this operation and re-initializing with the -pgp-keys flag\nto protect the returned unseal keys along with -backup to allow recovery of\nthe encrypted keys in case of emergency. You can delete the stored keys later\nusing the -delete flag.\n\nKey                      Value\n---                      -----\nNonce                    e0bd8648-0f8a-8641-bc18-2b4158d406a9\nStarted                  true\nRekey Progress           0/3\nNew Shares               5\nNew Threshold            3\nVerification Required    false\n</code></pre> <p>Provide the Recovery keys to meet the key threshold. One example is shown below:</p> <pre><code>$ vault operator rekey -target=recovery\n\nRekey operation nonce: e0bd8648-0f8a-8641-bc18-2b4158d406a9\nUnseal Key (will be hidden):\nKey                      Value\n---                      -----\nNonce                    e0bd8648-0f8a-8641-bc18-2b4158d406a9\nStarted                  true\nRekey Progress           1/3\nNew Shares               5\nNew Threshold            3\nVerification Required    false\n</code></pre> <p>Once all 3 Recovery keys have been applied, it will generate new Recovery keys from the new Root key.</p>","tags":["hashicorp","vault"]},{"location":"insights/2024/26-04-2024/hashicorp-vault-keys-autounseal/#how-do-we-generate-a-new-root-token","title":"How do we generate a new Root token?","text":"<p>A new Root token can be created with the Recovery keys.</p> <pre><code>$ vault operator generate-root -init\nA One-Time-Password has been generated for you and is shown in the OTP field.\nYou will need this value to decode the resulting root token, so keep it safe.\nNonce         c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted       true\nProgress      0/3\nComplete      false\nOTP           BJ4MR81PaVRw7fjLZqBti5H7dkiS\nOTP Length    28\n</code></pre> <p>Provide the Recovery keys to meet the key threshold. One example is shown below:</p> <pre><code>vault operator generate-root\nOperation nonce: c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nUnseal Key (will be hidden):\nNonce       c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted     true\nProgress    1/3\nComplete    false\n</code></pre> <p>Once all 3 Recovery keys have been supplied, it will provided the encoded Root token as shown below:</p> <pre><code>vault operator generate-root\nOperation nonce: c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nUnseal Key (will be hidden):\nNonce            c6f98535-43de-cd7b-d4d4-fd8fb17fd381\nStarted          true\nProgress         3/3\nComplete         true\nEncoded Token    KjxHYxxhXB1ZDwQuQVcgOi8HOhUfWS5BDgYbGA\n</code></pre> <p>The OTP from step 1 can be used to decode the Root token.</p> <pre><code>vault operator generate-root \\\n&gt; -decode=KjxHYxxhXB1ZDwQuQVcgOi8HOhUfWS5BDgYbGA \\\n&gt; -otp=BJ4MR81PaVRw7fjLZqBti5H7dkiS\n</code></pre> <p>To recap: most Vault data is encrypted using the encryption key in the keyring; the keyring is encrypted by the root key; and the root key is encrypted by the unseal key.</p> <pre><code>vault operator generate-root \\\n -decode=OgUXeGpAYHYIPRFaAQcsDgYQDQIdJRM9V3M8KQ \\\n -otp=RsdV3r8BCIIlMnXbCiZLJtbyg2Nj\n</code></pre>","tags":["hashicorp","vault"]},{"location":"retrospectives/2023/17-04-2023/","title":"17 04 2023","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/17-04-2023/#retrospective","title":"Retrospective","text":"<ol> <li> <p>Git recursive error when setting up <code>.gitconfig</code> file <code>includeIf</code></p> <p>Failed to noticed how I was including the same file within the target file created.</p> </li> <li> <p>Bash error <code>unexpected EOF while looking for matching \"'</code></p> <p>I was only looking at the current line instead of the surrounding code I had previously added.  Missed closing quote for a variable that previously defined.</p> </li> </ol>","tags":["dont-repeat"]},{"location":"retrospectives/2023/19-07-2023/","title":"19 07 2023","text":"","tags":["git"]},{"location":"retrospectives/2023/19-07-2023/#retrospective","title":"Retrospective","text":"<ol> <li>Git committer email mismatch</li> </ol> <p>While trying to push to a development instance of GitLab, I encountered the following error:</p> <pre><code>remote: GitLab: Committer's email 'John.Doe@team.example.com' does not follow the pattern '@test\\.team\\.example\\.com$'\n</code></pre> <p>I had the correct email configured in my local and global <code>~/.gitconfig</code> file with multiple git profiles to target different environments.</p> <p>Running <code>git config user.email</code> within the repo displayed the correct email as per regex pattern. </p> <p>After some head scratching it occured to me that the initial commit was using <code>John.Doe@team.example.com</code> (I only setup the git profile for dev after the first commit!).</p> <p>Had to fix this by running:</p> <pre><code>git -c user.name=\"John Doe\" -c user.email=John.Doe@test.team.example.com commit --amend --reset-author\n</code></pre> <p>This will amend the last commit, resetting the author email to match the required pattern.</p>","tags":["git"]},{"location":"retrospectives/2023/21-04-2023/","title":"21 04 2023","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#retrospective","title":"Retrospective","text":"","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#stick-to-the-plan","title":"Stick to the plan","text":"<p>I've been trying to develop a programming  design flow that allows me to keep the bigger picture in mind while working on the tasks at hand. My current flow goes something like this once the requirements are clear:</p> <ol> <li>Create a skeleton of the codebase to see the end to end flow. This is meant to enable visualising the call flow, dependencies    etc. Avoid writing any implementation details. Focus on stringing things together to see enough of the program flow, specially    from a developer experience perspective e.g: How easy is to make an API call? </li> <li>Refine the APIs of the codebase to reflect the expected API calls you intend to make and use.</li> <li>Rinse and repeat until you have clear APIs.</li> </ol>","tags":["dont-repeat"]},{"location":"retrospectives/2023/21-04-2023/#where-i-went-wrong","title":"Where I went wrong","text":"<p>I faltered when I got caught up in wanting to get things to work and delving into the implementation details immediately instead of following the plan. The problem here is once you are focused on just making things work, the code design mindset moves out of your primary focus. Sure, you can come back and refactor but at times, there is a chain reaction of refactoring that could have been avoided. I'd say more times than not, this is what has happened to me.</p> <p>I also made the cardinal sin of not adding git commits because I had gone too far down the rabbit hole trying to make things work and left with a whale of diffs to categorically pick and commit.</p>","tags":["dont-repeat"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#api-design","title":"api-design","text":"<ul> <li>Adapter Pattern</li> <li>API Design</li> <li>Go Reusing HTTP Client</li> </ul>"},{"location":"tags/#client","title":"client","text":"<ul> <li>Go Context Timeout</li> <li>Go Reusing HTTP Client</li> </ul>"},{"location":"tags/#context","title":"context","text":"<ul> <li>Go Context Timeout</li> </ul>"},{"location":"tags/#database","title":"database","text":"<ul> <li>Database Index and Constraints</li> </ul>"},{"location":"tags/#dont-repeat","title":"dont-repeat","text":"<ul> <li>17 04 2023</li> <li>21 04 2023</li> </ul>"},{"location":"tags/#faq","title":"faq","text":"<ul> <li>SSL CA Certs</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>Multiple Git Profiles</li> <li>19 07 2023</li> </ul>"},{"location":"tags/#go","title":"go","text":"<ul> <li>Adapter Pattern</li> <li>Go Context Timeout</li> <li>Go Reusing HTTP Client</li> </ul>"},{"location":"tags/#hashicorp","title":"hashicorp","text":"<ul> <li>Hashicorp Vault Keys for Autounseal</li> </ul>"},{"location":"tags/#istio","title":"istio","text":"<ul> <li>Istio Egress TLS Origination</li> </ul>"},{"location":"tags/#linux","title":"linux","text":"<ul> <li>Linux Primer</li> </ul>"},{"location":"tags/#networking","title":"networking","text":"<ul> <li>Linux Primer</li> </ul>"},{"location":"tags/#ssl","title":"ssl","text":"<ul> <li>SSL CA Certs</li> </ul>"},{"location":"tags/#tls","title":"tls","text":"<ul> <li>Istio Egress TLS Origination</li> </ul>"},{"location":"tags/#vault","title":"vault","text":"<ul> <li>Hashicorp Vault Keys for Autounseal</li> </ul>"}]}